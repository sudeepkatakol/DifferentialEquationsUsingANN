{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots18\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims): \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) \\\n",
    "        * np.sqrt(2.0 / layer_dims[l - 1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autograd import grad\n",
    "def sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z)), Z\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z), Z\n",
    "\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z), Z\n",
    "\n",
    "def cos(Z):\n",
    "    return np.cos(Z), Z\n",
    "\n",
    "def arctan(Z):\n",
    "    return np.arctan(Z), Z\n",
    "\n",
    "def arctan_grad(Z):\n",
    "    return 1/(1 + Z**2)\n",
    "\n",
    "def softmax_help(Z):\n",
    "    shift = Z - np.max(Z, axis=0)\n",
    "    exps = np.exp(shift)\n",
    "    return exps / np.sum(exps, axis = 0)\n",
    "\n",
    "def softmax(Z):\n",
    "    return softmax_help(Z), Z\n",
    "\n",
    "def softmax_grad(A):\n",
    "    dA = np.zeros_like(A.T)\n",
    "    for i in range((A.T).shape[0]):\n",
    "        s = A.T[i]\n",
    "        s.reshape((-1, 1))\n",
    "        dA[i] = np.diag(np.diagflat(s) * np.dot(s, s.T))\n",
    "    return dA.T\n",
    "    \n",
    "def softplus(Z):\n",
    "    return np.log(np.exp(Z) + 1), Z\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation, dA_prev):\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        dA = np.multiply(A, 1 - A) * np.dot(W, dA_prev)  \n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "        dA = np.greater_equal(Z, 0) * np.dot(W, dA_prev)\n",
    "    \n",
    "    elif activation == \"none\":\n",
    "        A, activation_cache = Z, Z\n",
    "        dA = np.dot(W, dA_prev)\n",
    "    \n",
    "    elif activation == \"tanh\":\n",
    "        A, activation_cache = tanh(Z)\n",
    "        dA = (np.ones_like(A) - A**2) * np.dot(W, dA_prev)\n",
    "        \n",
    "    elif activation == \"cos\":\n",
    "        A, activation_cache = cos(Z)\n",
    "        dA = - np.sin(Z) * np.dot(W, dA_prev)\n",
    "    \n",
    "    elif activation == \"softplus\":\n",
    "        A, activation_cache = softplus(Z)\n",
    "        dA =  sigmoid(Z)[0] * np.dot(W, dA_prev)\n",
    "        \n",
    "    elif activation == \"softmax\":\n",
    "        A, activation_cache = softmax(Z)\n",
    "        dA = softmax_grad(A) * np.dot(W, dA_prev)\n",
    "    \n",
    "    elif activation == \"arctan\":\n",
    "        A, activation_cache = arctan(Z)\n",
    "        dA = arctan_grad(Z) * np.dot(W, dA_prev)\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    assert (dA.shape == A.shape)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache, dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0625,  0.0625,  0.0625,  0.0625])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([2, 2, 2, 2])\n",
    "A = softmax_help(A)\n",
    "A.reshape((-1, 1))\n",
    "np.diag(np.diagflat(A) * np.dot(A, A.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.84772049e-04,   6.25000000e-02],\n",
       "       [  2.40505978e-03,   6.25000000e-02],\n",
       "       [  6.53763030e-03,   6.25000000e-02],\n",
       "       [  9.70270366e-01,   6.25000000e-02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 2],[2,2], [3,2], [8, 2]])\n",
    "softmax_grad(softmax_help(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.02738505e-04,   2.50000000e-01],\n",
       "       [  2.45389767e-03,   2.50000000e-01],\n",
       "       [  6.67038546e-03,   2.50000000e-01],\n",
       "       [  9.89972978e-01,   2.50000000e-01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_help(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters, activation):\n",
    "    caches = []\n",
    "    A = X\n",
    "    dA = np.ones_like(A)\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        dA_prev = dA\n",
    "        A, cache, dA = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation, dA_prev)\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache, dAL = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"none\", dA)\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (parameters['W' + str(L)].shape[0], X.shape[1]))\n",
    "    assert(dAL.shape == AL.shape)        \n",
    "    return AL, caches, dAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, activation_cache):\n",
    "    y = sigmoid(activation_cache)[0]\n",
    "    return np.multiply(dA, np.multiply(y, 1 - y))\n",
    "\n",
    "def relu_backward(dA, activation_cache):\n",
    "    y = np.greater_equal(activation_cache, 0)\n",
    "    return np.multiply(dA, y)\n",
    "\n",
    "def none_backward(dA, activation_cache):\n",
    "    return dA\n",
    "\n",
    "def cos_backward(dA, activation_cache):\n",
    "    return np.multiply(dA, -np.sin(activation_cache))\n",
    "\n",
    "def tanh_backward(dA, activation_cache):\n",
    "    return np.multiply(dA, np.ones_like(dA) - np.tanh(activation_cache)**2)\n",
    "\n",
    "def softplus_backward(dA, activation_cache):\n",
    "    return np.multiply(dA, sigmoid(activation_cache)[0])\n",
    "\n",
    "def softmax_backward(dA, activation_cache):\n",
    "    return np.multiply(dA , softmax_grad(softmax(activation_cache)[0]))\n",
    "\n",
    "def arctan_backward(dA, activation_cache):\n",
    "    return np.multiply(dA, arctan_grad(activation_cache))\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, cache[0].T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    \n",
    "    elif activation == \"none\":\n",
    "        dZ = none_backward(dA, activation_cache)\n",
    "    \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, activation_cache)\n",
    "        \n",
    "    elif activation == \"cos\":\n",
    "        dZ = cos_backward(dA, activation_cache)\n",
    "    \n",
    "    elif activation == \"softplus\":\n",
    "        dZ = softplus_backward(dA, activation_cache)\n",
    "    \n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "    \n",
    "    elif activation == \"arctan\":\n",
    "        dZ = arctan_backward(dA, activation_cache)\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, dAL, caches, activation):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    \n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"none\")\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation)\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :   \n",
    "    L = len(parameters) // 2 \n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "\n",
    "        s[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
    "        s[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
    "    return v, s\n",
    "\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,\n",
    "                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n",
    "        \n",
    "        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
    "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
    "       \n",
    "        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n",
    "        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n",
    "        \n",
    "        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
    "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
    "       \n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s[\"dW\" + str(l + 1)] + epsilon)\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s[\"db\" + str(l + 1)] + epsilon)\n",
    "      \n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#1: y' = 1/x^2 - y/x  using Neural Networks with intital condition y(1) = 0\n",
    "#2: y' = y + (x^2)y using Neural Networks with intital condition y(0) = 1\n",
    "#3: y' = cosx - sinx - y, , y(0) = 2\n",
    "\n",
    "def solve_3():\n",
    "    def dy_dx(x, y):\n",
    "        return np.cos(x) - np.sin(x) - y\n",
    "    \n",
    "    def analytic_solution(x):\n",
    "        return np.cos(x) + np.exp(-x)\n",
    "    \n",
    "    def predicted_soln(X, AL):\n",
    "        return 2+AL*X\n",
    "    \n",
    "    def cost(AL, dAL, X):\n",
    "        Y = 2+AL*X\n",
    "        term1 = dy_dx(X, Y)\n",
    "        term2 = AL + X*dAL\n",
    "        cost = np.mean((term1 - term2)**2)\n",
    "        cost = np.squeeze(cost)\n",
    "        return cost\n",
    "    \n",
    "    def compute_cost(AL, dAL, X):\n",
    "        return cost(AL, dAL, X), grad(cost)(AL, dAL, X)\n",
    "    \n",
    "    batch_size = 200\n",
    "    \n",
    "    def get_data():\n",
    "        h = np.random.uniform(-1,1,batch_size)\n",
    "        h.shape = (1, batch_size)\n",
    "        return h\n",
    "    \n",
    "    return get_data, predicted_soln, compute_cost, analytic_solution\n",
    "\n",
    "def solve_2():\n",
    "    def dy_dx(x, y):\n",
    "        return y*(1 + x**2)\n",
    "    \n",
    "    def analytic_solution(x):\n",
    "        return np.exp(x + x**3/3)\n",
    "    \n",
    "    def predicted_soln(X, AL):\n",
    "        return 1+AL*X\n",
    "    \n",
    "    def cost(AL, dAL, X):\n",
    "        Y = 1+AL*X\n",
    "        term1 = dy_dx(X, Y)\n",
    "        term2 = AL + X*dAL\n",
    "        cost = np.mean((term1 - term2)**2)\n",
    "        cost = np.squeeze(cost)\n",
    "        return cost\n",
    "    \n",
    "    def compute_cost(AL, dAL, X):\n",
    "        return cost(AL, dAL, X), grad(cost)(AL, dAL, X)\n",
    "    \n",
    "    batch_size = 100\n",
    "    \n",
    "    def get_data():\n",
    "        h = np.random.uniform(-0.5,0.5,batch_size)\n",
    "        h.shape = (1, batch_size)\n",
    "        return h\n",
    "    \n",
    "    return get_data, predicted_soln, compute_cost, analytic_solution\n",
    "    \n",
    "def solve_1():\n",
    "    def analytic_solution(x):\n",
    "        return np.log(x)/x\n",
    "\n",
    "    def dy_dx(x, y):\n",
    "        return 1/(x**2) - y/x\n",
    "    \n",
    "    def predicted_soln(X, AL):\n",
    "        return AL*(X - 1)\n",
    "        \n",
    "    def cost(AL, dAL, X):\n",
    "        Y = AL*(X - 1)\n",
    "        term1 = dy_dx(X, Y)\n",
    "        term2 = AL + (X-1)*dAL\n",
    "        cost = np.mean((term1 - term2)**2)\n",
    "        cost = np.squeeze(cost)\n",
    "        return cost\n",
    "    \n",
    "    def compute_cost(AL, dAL, X):\n",
    "        return cost(AL, dAL, X), grad(cost)(AL, dAL, X)\n",
    "    \n",
    "    batch_size = 500\n",
    "    \n",
    "    def get_data():\n",
    "        h = np.random.uniform(0.5,2.5,batch_size)\n",
    "        h.shape = (1, batch_size)\n",
    "        return h\n",
    "    \n",
    "    return get_data, predicted_soln, compute_cost, analytic_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.723658\n",
      "Cost after iteration 100: 0.084113\n",
      "Cost after iteration 200: 0.047326\n",
      "Cost after iteration 300: 0.019822\n",
      "Cost after iteration 400: 0.010623\n",
      "Cost after iteration 500: 0.006296\n",
      "Cost after iteration 600: 0.003920\n",
      "Cost after iteration 700: 0.004603\n",
      "Cost after iteration 800: 0.003708\n",
      "Cost after iteration 900: 0.002964\n",
      "Cost after iteration 1000: 0.001587\n",
      "Cost after iteration 1100: 0.002182\n",
      "Cost after iteration 1200: 0.001892\n",
      "Cost after iteration 1300: 0.001630\n",
      "Cost after iteration 1400: 0.002117\n",
      "Cost after iteration 1500: 0.001274\n",
      "Cost after iteration 1600: 0.001445\n",
      "Cost after iteration 1700: 0.001419\n",
      "Cost after iteration 1800: 0.001445\n",
      "Cost after iteration 1900: 0.001120\n",
      "Cost after iteration 2000: 0.001278\n",
      "Cost after iteration 2100: 0.001116\n",
      "Cost after iteration 2200: 0.000906\n",
      "Cost after iteration 2300: 0.000908\n",
      "Cost after iteration 2400: 0.001538\n",
      "Cost after iteration 2500: 0.000949\n",
      "Cost after iteration 2600: 0.001173\n",
      "Cost after iteration 2700: 0.000980\n",
      "Cost after iteration 2800: 0.001250\n",
      "Cost after iteration 2900: 0.001044\n",
      "Cost after iteration 3000: 0.000950\n",
      "Cost after iteration 3100: 0.001108\n",
      "Cost after iteration 3200: 0.000717\n",
      "Cost after iteration 3300: 0.000637\n",
      "Cost after iteration 3400: 0.000909\n",
      "Cost after iteration 3500: 0.000679\n",
      "Cost after iteration 3600: 0.000626\n",
      "Cost after iteration 3700: 0.000934\n",
      "Cost after iteration 3800: 0.000729\n",
      "Cost after iteration 3900: 0.001005\n",
      "Cost after iteration 4000: 0.001031\n",
      "Cost after iteration 4100: 0.001106\n",
      "Cost after iteration 4200: 0.000670\n",
      "Cost after iteration 4300: 0.000626\n",
      "Cost after iteration 4400: 0.000858\n"
     ]
    }
   ],
   "source": [
    "get_data, predicted, compute_cost, analytic_soln = solve_2()\n",
    "\n",
    "layers_dims = [1, 4, 4, 4, 1]\n",
    "\n",
    "L = len(layers_dims)             # number of layers in the neural networks\n",
    "costs = []                       # to keep track of the cost\n",
    "t = 0                            # initializing the counter required for Adam update\n",
    "seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "\n",
    "num_epochs = 4401\n",
    "\n",
    "parameters = initialize_parameters_deep(layers_dims)\n",
    "v, s = initialize_adam(parameters)\n",
    "\n",
    "activation = 'sigmoid'\n",
    "\n",
    "print_cost = True\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    X = get_data()\n",
    "    \n",
    "    AL, caches, dAL = L_model_forward(X, parameters, activation)\n",
    "    \n",
    "    cost, dAL = compute_cost(AL, dAL, X)\n",
    "   \n",
    "    grads = L_model_backward(AL, dAL, caches, activation)\n",
    "    \n",
    "    '''\n",
    "    # Perform check\n",
    "    Y = dx_dt(AL)\n",
    "    gradient_checker(parameters, grads, Y, X)\n",
    "    break\n",
    "    '''\n",
    "    \n",
    "    t = t + 1\n",
    "    parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t)\n",
    "    \n",
    "    if print_cost and i % 100 == 0:\n",
    "        print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "    if print_cost and i % 100 == 0:\n",
    "        costs.append(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa735608208>,\n",
       " <matplotlib.lines.Line2D at 0x7fa735608940>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAHVCAYAAADywj0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa737a14470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = np.linspace(-0.5, 0.5, 21) #use for eqn2 and for eqn3\n",
    "#test = np.linspace(0.5, 2.5, 21) #use for eqn1\n",
    "test.shape = (1, 21)\n",
    "approx = predicted(L_model_forward(test, parameters, activation)[0], test)\n",
    "test.shape = (21, )\n",
    "approx.shape = (21, )\n",
    "actual = np.squeeze(analytic_soln(test))\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(test, actual, test, approx)\n",
    "# plt.savefig(\"y' = y + (x^2)y, none.png\")\n",
    "#plt.savefig(\"y' = cosx - sinx - y, none.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dy_dx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-d4339f748d9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdAL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mterm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdy_dx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mterm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdAL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mterm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dy_dx' is not defined"
     ]
    }
   ],
   "source": [
    "test.shape = (1, 11)\n",
    "AL, _, dAL = L_model_forward(test, parameters, activation)\n",
    "Y = AL*(test - 1)\n",
    "term1 = dy_dx(test, Y)\n",
    "term2 = AL + (test-1)*dAL\n",
    "term1.shape = (11, )\n",
    "term2.shape = (11, )\n",
    "test.shape = (11, )\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(test, term1, test, term2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
