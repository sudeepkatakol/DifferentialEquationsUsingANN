{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_random_params(layer_sizes, rs=np.random.RandomState(0)):\n",
    "    params = []\n",
    "    for m, n in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "        e = np.sqrt(6/(m + n))\n",
    "        params.append([2 * e * rs.rand(n, m) - e,   # weight matrix\n",
    "             2 * e * rs.rand(n, 1) - e]) # bias vector\n",
    "    return np.array(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "def get_data():\n",
    "    t = np.linspace(-0.5,0.5,num=batch_size, endpoint=True)\n",
    "    t.shape = (1, batch_size)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# Return the softplus of variable x\n",
    "def softplus(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Choose activation\n",
    "activation = softplus\n",
    "activation_grad = sigmoid\n",
    "\n",
    "def forward_prop(params, inputs):\n",
    "    fp = []\n",
    "    fp.append([inputs])\n",
    "    for W, b in params:\n",
    "        z = np.dot(W, inputs) + b\n",
    "        inputs = activation(z)\n",
    "        fp.append([z, inputs])\n",
    "    del fp[-1][-1]\n",
    "    return fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function(y_hat, y):\n",
    "    return np.sum((y - y_hat) ** 2)/ (2 * y_hat.shape[1])\n",
    "\n",
    "def dE_dy_hat(y_hat, y):\n",
    "    return (y_hat - y) / y_hat.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_prop(y, params, forward_prop):\n",
    "    # delta (l) = dE_dz(l)\n",
    "    # deltas also give the derivate with respect to the bias paramaters\n",
    "    deltas = [0 for _ in range(len(params) + 1)]\n",
    "    # print(deltas)\n",
    "    \n",
    "    grad_params = [0 for _ in range(len(params))]\n",
    "    \n",
    "    deltas[-1] = dE_dy_hat(forward_prop[-1][0], y)\n",
    "    \n",
    "    for i in range(-2, -1*(len(forward_prop)) - 1, -1):\n",
    "        deltas[i] = np.multiply(np.dot(np.transpose(params[i+1][0]), deltas[i+1]), activation_grad(forward_prop[i][0]))\n",
    "        grad_params[i+1] = [np.dot(deltas[i+1], np.transpose(forward_prop[i][-1])), np.reshape(np.sum(deltas[i+1], axis = 1),(deltas[i+1].shape[0],1))] \n",
    "    return np.array(grad_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "https://www.coursera.org/learn/machine-learning/resources/EcbzQ\n",
    "epsilon = 1e-4;\n",
    "for i = 1:n,\n",
    "  thetaPlus = theta;\n",
    "  thetaPlus(i) += epsilon;\n",
    "  thetaMinus = theta;\n",
    "  thetaMinus(i) -= epsilon;\n",
    "  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)\n",
    "end\n",
    "'''\n",
    "def gradient_checker(params, grad_params, y, inputs):\n",
    "    epsilon = 10**(-4)\n",
    "    for i in range(len(params)):\n",
    "        for j in range(len(params[i])):\n",
    "            if params[i][j].shape[1] > 1:\n",
    "                for k in range(len(params[i][j])):\n",
    "                    for l in range(len(params[i][j][k])):\n",
    "                        params[i][j][k][l] += epsilon\n",
    "                        y_hat = forward_prop(params, inputs)[-1][0]\n",
    "                        J_theta_plus = cost_function(y_hat, y)\n",
    "                        params[i][j][k][l] -= (2 * epsilon)\n",
    "                        y_hat = forward_prop(params, inputs)[-1][0]\n",
    "                        J_theta_minus = cost_function(y_hat, y)\n",
    "                        params[i][j][k][l] += epsilon\n",
    "                        grad_approx = (J_theta_plus - J_theta_minus) / (2 * epsilon)\n",
    "                        print(abs(grad_params[i][j][k][l] - grad_approx) * 100 / abs(grad_params[i][j][k][l]))\n",
    "            else:\n",
    "                for k in range(len(params[i][j])):\n",
    "                    params[i][j][k] += epsilon\n",
    "                    y_hat = forward_prop(params, inputs)[-1][0]\n",
    "                    J_theta_plus = cost_function(y_hat, y)\n",
    "                    params[i][j][k] -= (2 * epsilon)\n",
    "                    y_hat = forward_prop(params, inputs)[-1][0]\n",
    "                    J_theta_minus = cost_function(y_hat, y)\n",
    "                    params[i][j][k] += epsilon\n",
    "                    grad_approx = (J_theta_plus - J_theta_minus) / (2 * epsilon)\n",
    "                    print(abs(grad_params[i][j][k] - grad_approx) * 100 / abs(grad_params[i][j][k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "g_t^(2) indicates the elementwise\n",
    "square g_t \f",
    " g_t. Good default settings for the tested machine learning problems are α = 0.001,\n",
    "β1 = 0.9, β2 = 0.999 and e = 10−8\n",
    ". All operations on vectors are element-wise\n",
    "Require: α: Stepsize\n",
    "Require: β1, β2 ∈ [0, 1): Exponential decay rates for the moment estimates\n",
    "Require: f(θ): Stochastic objective function with parameters θ\n",
    "Require: θ0: Initial parameter vector\n",
    "m0 ← 0 (Initialize 1st moment vector)\n",
    "v0 ← 0 (Initialize 2nd moment vector)\n",
    "t ← 0 (Initialize timestep)\n",
    "while θt not converged do\n",
    "t ← t + 1\n",
    "gt ← ∇θft(θt−1) (Get gradients w.r.t. stochastic objective at timestep t)\n",
    "mt ← β1 · mt−1 + (1 − β1) · gt (Update biased first moment estimate)\n",
    "vt ← β2 · vt−1 + (1 − β2) · gt**2\n",
    "(Update biased second raw moment estimate)\n",
    "mbt ← mt/(1 − β1^t) (Compute bias-corrected first moment estimate)\n",
    "vbt ← vt/(1 − β2^t)\n",
    "\n",
    "(Compute bias-corrected second raw moment estimate)\n",
    "θt ← θt−1 − α · mbt/(√vbt + e) (Update parameters)\n",
    "end while\n",
    "return θt (Resulting parameters)\n",
    "'''\n",
    "def adam_optimization(inputs, y, params, no_of_steps=1000, alpha = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = 10**(-8)):\n",
    "    m = 0\n",
    "    v = 0\n",
    "    for i in range(1, no_of_steps+1):\n",
    "        g = backward_prop(y, params, forward_prop(params, inputs))\n",
    "        m = beta1 * m + (1- beta1)*g\n",
    "        v = beta2 * v + (1- beta2)*(g**2)\n",
    "        m_tilde = m/(1 - beta1**i)\n",
    "        v_tilde = v/(1 - beta2**i)\n",
    "        sub = alpha * m_tilde / (v_tilde + epsilon)**(0.5)\n",
    "        params -= sub\n",
    "        if i%100 == 0:\n",
    "            print(cost_function(y, forward_prop(params, inputs)[-1][0]))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.36406008348\n",
      "2.19626046916\n",
      "1.58291809595\n",
      "1.20989995796\n",
      "0.961737887296\n",
      "0.786818014374\n",
      "0.658637891705\n",
      "0.562133868535\n",
      "0.488067943424\n",
      "0.43043755421\n",
      "0.385162954139\n",
      "0.349369139226\n",
      "0.320967792082\n",
      "0.298401506665\n",
      "0.280481144018\n",
      "0.266279346716\n",
      "0.255059326327\n",
      "0.246226540281\n",
      "0.239295575785\n",
      "0.233867256156\n",
      "0.229612594108\n",
      "0.22626123029\n",
      "0.223592685824\n",
      "0.221429270289\n",
      "0.219629891371\n",
      "0.218084335145\n",
      "0.216707831646\n",
      "0.215435887579\n",
      "0.2142194592\n",
      "0.213020562974\n",
      "0.211808395863\n",
      "0.210555980507\n",
      "0.209237282327\n",
      "0.207824680666\n",
      "0.206286624344\n",
      "0.204585267823\n",
      "0.202673869131\n",
      "0.200493737402\n",
      "0.197970555511\n",
      "0.19500999743\n",
      "0.191492769088\n",
      "0.187269647716\n",
      "0.18215799034\n",
      "0.175942705361\n",
      "0.168386105517\n",
      "0.159247831811\n",
      "0.148294609894\n",
      "0.135216367293\n",
      "0.119225094009\n",
      "0.097881185731\n",
      "0.0686785090559\n",
      "0.0486242194945\n",
      "0.0395930476521\n",
      "0.0335064675969\n",
      "0.0289075545563\n",
      "0.0253666305552\n",
      "0.0226234566032\n",
      "0.0204722673257\n",
      "0.0187450181117\n",
      "0.0173149974525\n",
      "0.0160941265327\n",
      "0.0150222323983\n",
      "0.0140552160994\n",
      "0.0131562506718\n",
      "0.0122901576631\n",
      "0.0114197859062\n",
      "0.0105041498325\n",
      "0.0095010761607\n",
      "0.00838279238831\n",
      "0.00717496162023\n",
      "0.0059975549111\n",
      "0.00501988586517\n",
      "0.004324424391\n",
      "0.00386513864077\n",
      "0.00355407306998\n",
      "0.00332749121874\n",
      "0.00315179613174\n",
      "0.00301037317081\n",
      "0.00289407072763\n",
      "0.00279697004867\n",
      "0.00271478689541\n",
      "0.00264424844887\n",
      "0.00258280017021\n",
      "0.00252843278185\n",
      "0.00247956320206\n"
     ]
    }
   ],
   "source": [
    "init_params = init_random_params([2, 4, 3, 2])\n",
    "x = np.random.RandomState(0).randn(2, 10)\n",
    "y = np.array([np.sin(x[0]), np.cos(x[1])])\n",
    "optimized_params = adam_optimization(x, y, init_params, no_of_steps=8500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.88355816,  0.39934334,  0.83504008,  0.8914434 ,  0.94544585,\n",
       "        -0.82423043,  0.80687031, -0.14928999, -0.11735898,  0.39314023],\n",
       "       [ 0.99431747,  0.11404552,  0.63592369,  1.03702934,  0.92463925,\n",
       "         1.00307311,  0.11018637,  0.87594322,  0.94010261,  0.705511  ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_prop(inputs=x, params=optimized_params)[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9813841 ,  0.38956314,  0.82979374,  0.78376151,  0.95628847,\n",
       "        -0.82897801,  0.81346693, -0.15077996, -0.10303566,  0.39915816],\n",
       "       [ 0.98964365,  0.11625932,  0.72412071,  0.99260672,  0.90309941,\n",
       "         0.94484532,  0.07664202,  0.97902875,  0.95139326,  0.65690057]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
